#!/usr/bin/env bash
set -eo pipefail

# Disable strict variable checking temporarily for conda activation
set +u

# Activate conda env
# Try to find and activate conda automatically
if command -v conda >/dev/null 2>&1; then
  eval "$(conda shell.bash hook)" || true
elif [ -f "$HOME/miniconda3/bin/conda" ]; then
  eval "$($HOME/miniconda3/bin/conda shell.bash hook)" || true
elif [ -f "/opt/miniconda3/bin/conda" ]; then
  eval "$(/opt/miniconda3/bin/conda shell.bash hook)" || true
fi
conda activate seg >/dev/null 2>&1

# Re-enable strict mode after conda activation
set -u

# Project-rooted nnUNet paths (use relative to script location)
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
export nnUNet_raw="$PROJECT_DIR/data/nnUNet_raw"
export nnUNet_preprocessed="$PROJECT_DIR/data/nnUNet_preprocessed"
export nnUNet_results="$PROJECT_DIR/data/nnUNet_results"

# Optimized memory settings for 8GB GPU
export nnUNet_n_proc_DA="4"
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True,max_split_size_mb:64"
export CUDA_LAUNCH_BLOCKING="1"
export PYTORCH_CUDA_ALLOC_CONF="backend:native"

# Check if checkpoint exists
CHECKPOINT_DIR="$nnUNet_results/Dataset803_TA/nnUNetTrainerLightMUNet__nnUNetPlans__2d/fold_all"
if [ ! -d "$CHECKPOINT_DIR" ]; then
    echo "âŒ æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨: $CHECKPOINT_DIR"
    echo "   è¯·å…ˆè¿è¡Œ start_training.sh å¼€å§‹è®­ç»ƒ"
    exit 1
fi

if [ ! -f "$CHECKPOINT_DIR/checkpoint_best.pth" ] && [ ! -f "$CHECKPOINT_DIR/checkpoint_latest.pth" ]; then
    echo "âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼"
    echo "   æ£€æŸ¥ç‚¹ç›®å½•: $CHECKPOINT_DIR"
    echo "   è¯·å…ˆè¿è¡Œ start_training.sh å¼€å§‹è®­ç»ƒ"
    exit 1
fi

# Clear GPU cache
python -c "import torch; torch.cuda.empty_cache() if torch.cuda.is_available() else None"

echo "ğŸ”„ ç»§ç»­è®­ç»ƒé…ç½®:"
echo "   GPU Memory: 8GB RTX 4060 Laptop"
echo "   è®­ç»ƒæ¨¡å¼: ç»§ç»­è®­ç»ƒ (ä½¿ç”¨ç°æœ‰æ£€æŸ¥ç‚¹)"
echo "   æ£€æŸ¥ç‚¹ç›®å½•: $CHECKPOINT_DIR"
echo "   Batch Size: è‡ªåŠ¨ä¼˜åŒ–"
echo "   Memory Strategy: ä¿å®ˆåˆ†é…ç­–ç•¥"

# Show environment summary
python - << 'PY'
import torch, os
print('CUDA available:', torch.cuda.is_available())
if torch.cuda.is_available():
    print(f'GPU: {torch.cuda.get_device_name(0)}')
    print(f'Total Memory: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB')
    print(f'Available Memory: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0))/1024**3:.1f}GB')
print('nnUNet_n_proc_DA:', os.environ.get('nnUNet_n_proc_DA'))
print('PYTORCH_CUDA_ALLOC_CONF:', os.environ.get('PYTORCH_CUDA_ALLOC_CONF'))
PY

# Check latest checkpoint info
if [ -f "$CHECKPOINT_DIR/checkpoint_latest.pth" ]; then
    echo ""
    echo "ğŸ“Š æœ€æ–°æ£€æŸ¥ç‚¹ä¿¡æ¯:"
    python - << PY
import torch
try:
    ckpt = torch.load("$CHECKPOINT_DIR/checkpoint_latest.pth", map_location='cpu')
    if 'current_epoch' in ckpt:
        print(f"   ä¸Šæ¬¡è®­ç»ƒè½®æ¬¡: Epoch {ckpt['current_epoch']}")
    if 'ema_fg_dice' in ckpt['network_weights']:
        print(f"   EMA Dice: {ckpt['network_weights']['ema_fg_dice']:.4f}")
except Exception as e:
    print(f"   æ— æ³•è¯»å–æ£€æŸ¥ç‚¹ä¿¡æ¯: {e}")
PY
fi

# Stop any stale trainings and clear GPU memory
echo ""
echo "ğŸ§¹ æ¸…ç† GPU è¿›ç¨‹..."
pkill -f "nnUNetv2_train 803 2d all -tr nnUNetTrainerLightMUNet" || true
sleep 3

# Clear any GPU memory
python -c "import torch; torch.cuda.empty_cache() if torch.cuda.is_available() else None"

# Launch continuation training with -c flag
LOG_OUT="/tmp/nnunet_resume_training_$(date +%Y%m%d_%H%M%S).out"
echo "ğŸš€ ç»§ç»­ 2D åˆ†å‰²è®­ç»ƒ..."
echo "   ä»ä¸Šæ¬¡ä¸­æ–­å¤„æ¢å¤è®­ç»ƒ"

# Launch training WITH continuation flag (--c)
CUDA_VISIBLE_DEVICES=0 nohup nnUNetv2_train 803 2d all -tr nnUNetTrainerLightMUNet --c -device cuda >> "$LOG_OUT" 2>&1 &
PID=$!

echo ""
echo "âœ… è®­ç»ƒå·²æ¢å¤ PID=$PID"
echo "ğŸ“ ä¸´æ—¶æ—¥å¿—: $LOG_OUT"
echo "ğŸ“Š å®æ—¶ç›‘æ§: tail -f $LOG_OUT"
echo "ğŸ“‚ è®­ç»ƒæ—¥å¿—: $CHECKPOINT_DIR/training_log_*.txt"

echo ""
echo "ğŸ¯ ç»§ç»­è®­ç»ƒç‰¹ç‚¹:"
echo "   âœ… ä»ä¸Šæ¬¡ä¸­æ–­çš„ epoch ç»§ç»­"
echo "   âœ… ä¿ç•™æ‰€æœ‰è®­ç»ƒå†å²å’Œæ£€æŸ¥ç‚¹"
echo "   âœ… è‡ªåŠ¨åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡"
echo "   âœ… å†…å­˜ä¼˜åŒ–é…ç½® (4è¿›ç¨‹, 64MBåˆ†å‰²)"
echo ""
echo "ğŸ›‘ åœæ­¢è®­ç»ƒ: kill $PID"
echo "ğŸ è®­ç»ƒçŠ¶æ€æ£€æŸ¥: tail -20 $CHECKPOINT_DIR/training_log_*.txt"